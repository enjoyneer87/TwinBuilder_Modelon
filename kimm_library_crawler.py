#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œêµ­ê¸°ê³„ì—°êµ¬ì› ë„ì„œê´€ API í¬ë¡¤ëŸ¬
ì—°êµ¬ë…¼ë¬¸ê³¼ ì—°êµ¬ë³´ê³ ì„œ ëª©ë¡ì„ í¬ë¡¤ë§í•˜ì—¬ ì—‘ì…€ë¡œ ì¶”ì¶œ

API URL: https://library.kimm.re.kr/openAPI/openAPI_allsearch.do
ì‘ì„±ì¼: 2025ë…„ 8ì›” 29ì¼
"""

import requests
import pandas as pd
import time
import json
from datetime import datetime
import logging
from typing import List, Dict, Optional
import urllib.parse

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('kimm_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class KIMMLibraryCrawler:
    """í•œêµ­ê¸°ê³„ì—°êµ¬ì› ë„ì„œê´€ API í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.base_url = "https://library.kimm.re.kr/openAPI/openAPI_allsearch.do"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.data = []
        
    def search_documents(self, doc_type: str = "all", start_page: int = 1, max_pages: int = None) -> List[Dict]:
        """
        ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            doc_type: ë¬¸ì„œ íƒ€ì… ("paper" - ì—°êµ¬ë…¼ë¬¸, "report" - ì—°êµ¬ë³´ê³ ì„œ, "all" - ì „ì²´)
            start_page: ì‹œì‘ í˜ì´ì§€
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘: íƒ€ì…={doc_type}, ì‹œì‘í˜ì´ì§€={start_page}")
        
        results = []
        page = start_page
        total_pages = None
        
        while True:
            try:
                # API íŒŒë¼ë¯¸í„° ì„¤ì •
                params = {
                    'page': page,
                    'rows': 50,  # í•œ í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜
                    'format': 'json'  # JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ìš”ì²­
                }
                
                # ë¬¸ì„œ íƒ€ì…ë³„ í•„í„° ì¶”ê°€
                if doc_type == "paper":
                    params['type'] = 'paper'
                elif doc_type == "report":
                    params['type'] = 'report'
                
                logger.info(f"í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")
                
                # API ìš”ì²­
                response = self.session.get(self.base_url, params=params, timeout=30)
                response.raise_for_status()
                
                # ì‘ë‹µ ì²˜ë¦¬
                if response.headers.get('content-type', '').startswith('application/json'):
                    data = response.json()
                else:
                    # JSONì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„
                    text = response.text
                    logger.warning(f"JSONì´ ì•„ë‹Œ ì‘ë‹µ ìˆ˜ì‹ : {text[:200]}...")
                    
                    # ê°„ë‹¨í•œ HTML íŒŒì‹± ì‹œë„
                    results_page = self.parse_html_response(text)
                    if results_page:
                        results.extend(results_page)
                    else:
                        logger.warning(f"í˜ì´ì§€ {page}ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ")
                        break
                
                # JSON ì‘ë‹µ ì²˜ë¦¬
                if 'data' in locals() and isinstance(data, dict):
                    if 'results' in data and data['results']:
                        results.extend(data['results'])
                        
                        # ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸
                        if total_pages is None and 'totalPages' in data:
                            total_pages = data['totalPages']
                            logger.info(f"ì´ í˜ì´ì§€ ìˆ˜: {total_pages}")
                    else:
                        logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìŒ")
                        break
                
                # ìµœëŒ€ í˜ì´ì§€ í™•ì¸
                if max_pages and page >= max_pages:
                    logger.info(f"ìµœëŒ€ í˜ì´ì§€({max_pages})ì— ë„ë‹¬")
                    break
                
                if total_pages and page >= total_pages:
                    logger.info(f"ë§ˆì§€ë§‰ í˜ì´ì§€({total_pages})ì— ë„ë‹¬")
                    break
                
                page += 1
                
                # API ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                time.sleep(1)
                
            except requests.exceptions.RequestException as e:
                logger.error(f"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
                break
            except json.JSONDecodeError as e:
                logger.error(f"í˜ì´ì§€ {page} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                break
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        logger.info(f"ì´ {len(results)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ")
        return results
    
    def parse_html_response(self, html_text: str) -> List[Dict]:
        """HTML ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_text, 'html.parser')
            
            results = []
            # ì‹¤ì œ HTML êµ¬ì¡°ì— ë”°ë¼ íŒŒì‹± ë¡œì§ êµ¬í˜„
            # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ì¸ ì˜ˆì‹œë§Œ ì œê³µ
            
            return results
            
        except ImportError:
            logger.warning("BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. HTML íŒŒì‹± ë¶ˆê°€")
            return []
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    def search_with_keywords(self, keywords: List[str]) -> List[Dict]:
        """í‚¤ì›Œë“œë³„ ê²€ìƒ‰"""
        all_results = []
        
        for keyword in keywords:
            logger.info(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
            
            params = {
                'query': keyword,
                'page': 1,
                'rows': 100,
                'format': 'json'
            }
            
            try:
                response = self.session.get(self.base_url, params=params, timeout=30)
                response.raise_for_status()
                
                if response.headers.get('content-type', '').startswith('application/json'):
                    data = response.json()
                    if 'results' in data:
                        all_results.extend(data['results'])
                
                time.sleep(1)  # API ê³¼ë¶€í•˜ ë°©ì§€
                
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return all_results
    
    def save_to_excel(self, data: List[Dict], filename: str = None) -> str:
        """ë°ì´í„°ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"KIMM_library_data_{timestamp}.xlsx"
        
        try:
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(data)
            
            if df.empty:
                logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # ì „ì²´ ë°ì´í„°
                df.to_excel(writer, sheet_name='ì „ì²´', index=False)
                
                # ë¬¸ì„œ íƒ€ì…ë³„ ì‹œíŠ¸ ìƒì„±
                if 'type' in df.columns:
                    for doc_type in df['type'].unique():
                        if pd.notna(doc_type):
                            type_df = df[df['type'] == doc_type]
                            sheet_name = f"{doc_type}"[:31]  # ì—‘ì…€ ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ
                            type_df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # ì—°ë„ë³„ ì‹œíŠ¸ ìƒì„± (ì¶œíŒì—°ë„ê°€ ìˆëŠ” ê²½ìš°)
                if 'year' in df.columns:
                    for year in sorted(df['year'].dropna().unique(), reverse=True):
                        year_df = df[df['year'] == year]
                        sheet_name = f"year_{int(year)}"
                        year_df.to_excel(writer, sheet_name=sheet_name, index=False)
            
            logger.info(f"ì—‘ì…€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            logger.info(f"ì´ {len(df)}ê°œ ë ˆì½”ë“œ ì €ì¥")
            
            return filename
            
        except Exception as e:
            logger.error(f"ì—‘ì…€ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def create_sample_data(self) -> List[Dict]:
        """APIê°€ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        logger.info("ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
        
        sample_data = [
            {
                'title': 'ë°°í„°ë¦¬ ì—´ê´€ë¦¬ ì‹œìŠ¤í…œ ì—°êµ¬',
                'author': 'ê¹€ì—°êµ¬',
                'type': 'paper',
                'year': 2024,
                'journal': 'í•œêµ­ê¸°ê³„í•™íšŒë…¼ë¬¸ì§‘',
                'keywords': 'ë°°í„°ë¦¬, ì—´ê´€ë¦¬, SOC',
                'abstract': 'ë°°í„°ë¦¬ ì—´ê´€ë¦¬ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„± í–¥ìƒì— ê´€í•œ ì—°êµ¬'
            },
            {
                'title': 'Modelicaë¥¼ ì´ìš©í•œ ë°°í„°ë¦¬ ëª¨ë¸ë§',
                'author': 'ë°•ëª¨ë¸',
                'type': 'report',
                'year': 2024,
                'institution': 'í•œêµ­ê¸°ê³„ì—°êµ¬ì›',
                'keywords': 'Modelica, ë°°í„°ë¦¬, ì‹œë®¬ë ˆì´ì…˜',
                'abstract': 'Modelica ì–¸ì–´ë¥¼ ì‚¬ìš©í•œ ë°°í„°ë¦¬ ëª¨ë¸ë§ ê¸°ë²• ì—°êµ¬'
            },
            {
                'title': 'SOC ì¶”ì • ì•Œê³ ë¦¬ì¦˜ ê°œë°œ',
                'author': 'ì´ì¶”ì •',
                'type': 'paper',
                'year': 2023,
                'journal': 'ì—ë„ˆì§€ê³µí•™íšŒì§€',
                'keywords': 'SOC, ì¶”ì •, ì¹¼ë§Œí•„í„°',
                'abstract': 'ì •í™•í•œ SOC ì¶”ì •ì„ ìœ„í•œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ'
            }
        ]
        
        return sample_data

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("KIMM ë„ì„œê´€ í¬ë¡¤ë§ ì‹œì‘")
    
    crawler = KIMMLibraryCrawler()
    
    try:
        # ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰ ì‹œë„
        logger.info("ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        all_docs = crawler.search_documents(doc_type="all", max_pages=10)
        
        # ì—°êµ¬ë…¼ë¬¸ ê²€ìƒ‰
        logger.info("ì—°êµ¬ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...")
        papers = crawler.search_documents(doc_type="paper", max_pages=5)
        
        # ì—°êµ¬ë³´ê³ ì„œ ê²€ìƒ‰
        logger.info("ì—°êµ¬ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘...")
        reports = crawler.search_documents(doc_type="report", max_pages=5)
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        keywords = ['ë°°í„°ë¦¬', 'SOC', 'Modelica', 'ì—´ê´€ë¦¬', 'ì‹œë®¬ë ˆì´ì…˜']
        keyword_results = crawler.search_with_keywords(keywords)
        
        # ëª¨ë“  ê²°ê³¼ í†µí•©
        all_results = all_docs + papers + reports + keyword_results
        
        if not all_results:
            logger.warning("APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
            all_results = crawler.create_sample_data()
        
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        seen_titles = set()
        unique_results = []
        for result in all_results:
            title = result.get('title', '')
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(result)
        
        logger.info(f"ì¤‘ë³µ ì œê±° í›„ {len(unique_results)}ê°œ ë¬¸ì„œ")
        
        # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
        excel_file = crawler.save_to_excel(unique_results)
        
        if excel_file:
            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {excel_file}")
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“ íŒŒì¼: {excel_file}")
            print(f"ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {len(unique_results)}")
        else:
            logger.error("ì—‘ì…€ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒ˜í”Œ ë°ì´í„°ë¼ë„ ì €ì¥
        sample_data = crawler.create_sample_data()
        excel_file = crawler.save_to_excel(sample_data, "KIMM_sample_data.xlsx")
        if excel_file:
            print(f"\nâš ï¸  API ì˜¤ë¥˜ë¡œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {excel_file}")

if __name__ == "__main__":
    main()
