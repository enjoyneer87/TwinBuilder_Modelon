{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb5d651",
   "metadata": {},
   "source": [
    "# KIMM 도서관 API 크롤러\n",
    "## 한국기계연구원 연구논문 및 연구보고서 수집\n",
    "\n",
    "**목표**: https://library.kimm.re.kr/openAPI/openAPI_allsearch.do API를 사용하여 연구논문과 연구보고서 목록을 크롤링하고 엑셀로 추출\n",
    "\n",
    "---\n",
    "*작성일: 2025년 8월 29일*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 필요한 라이브러리 설치\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✅ {package} 이미 설치됨\")\n",
    "    except ImportError:\n",
    "        print(f\"📦 {package} 설치 중...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✅ {package} 설치 완료\")\n",
    "\n",
    "# 필요한 패키지들 설치\n",
    "packages = ['requests', 'pandas', 'openpyxl', 'beautifulsoup4', 'lxml']\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n🎉 모든 라이브러리 설치 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6801db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 라이브러리 임포트\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"📚 라이브러리 임포트 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. KIMM 도서관 크롤러 클래스 정의\n",
    "class KIMMLibraryCrawler:\n",
    "    \"\"\"한국기계연구원 도서관 API 크롤러\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://library.kimm.re.kr/openAPI/openAPI_allsearch.do\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.data = []\n",
    "        print(\"[INIT] KIMM 도서관 크롤러 초기화 완료\")\n",
    "        \n",
    "    def test_api_connection(self):\n",
    "        \"\"\"API 연결 테스트\"\"\"\n",
    "        print(\"[TEST] API 연결 테스트 중...\")\n",
    "        try:\n",
    "            response = self.session.get(self.base_url, timeout=10)\n",
    "            print(f\"[TEST] 응답 상태: {response.status_code}\")\n",
    "            print(f\"[TEST] 응답 타입: {response.headers.get('content-type', 'Unknown')}\")\n",
    "            print(f\"[TEST] 응답 크기: {len(response.text)} bytes\")\n",
    "            \n",
    "            # 응답 내용 일부 출력\n",
    "            if response.text:\n",
    "                print(\"[TEST] 응답 내용 (첫 500자):\")\n",
    "                print(response.text[:500])\n",
    "            \n",
    "            return response.status_code == 200\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] API 연결 실패: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_all_documents(self, max_results=5000):\n",
    "        \"\"\"전체 문서 목록 가져오기 (키워드 없이)\"\"\"\n",
    "        print(f\"[RUN] 전체 데이터베이스 검색... (최대 {max_results}개)\")\n",
    "        \n",
    "        results = []\n",
    "        page = 1\n",
    "        per_page = 50  # 한 번에 더 많이 가져오기\n",
    "        \n",
    "        while len(results) < max_results:\n",
    "            try:\n",
    "                # 빈 쿼리로 전체 검색\n",
    "                params = {\n",
    "                    'page': page,\n",
    "                    'pageSize': per_page,\n",
    "                    'lang': 'kor'\n",
    "                }\n",
    "                \n",
    "                print(f\"[RUN] 페이지 {page} 요청... (누적 {len(results)}개)\")\n",
    "                response = self.session.get(self.base_url, params=params, timeout=30)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"[WARN] HTTP 오류: {response.status_code}\")\n",
    "                    break\n",
    "                \n",
    "                # JSON 응답 파싱\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    if 'xmlData' in data and 'list' in data['xmlData']:\n",
    "                        list_data = data['xmlData']['list']\n",
    "                        if list_data and 'rowList' in list_data[0]:\n",
    "                            page_results = self.parse_json_results(list_data[0]['rowList'])\n",
    "                            \n",
    "                            if not page_results:\n",
    "                                print(f\"[INFO] 페이지 {page} 더 이상 결과 없음\")\n",
    "                                break\n",
    "                            \n",
    "                            results.extend(page_results)\n",
    "                            print(f\"[OK] 페이지 {page}: {len(page_results)}개 수집\")\n",
    "                            \n",
    "                            # 전체 페이지 수 확인\n",
    "                            if 'pageing' in list_data[0]:\n",
    "                                total_pages_raw = list_data[0]['pageing'].get('totalPage', 0)\n",
    "                                try:\n",
    "                                    total_pages = int(float(total_pages_raw))\n",
    "                                except Exception:\n",
    "                                    total_pages = int(total_pages_raw) if str(total_pages_raw).isdigit() else 0\n",
    "                                total_docs = list_data[0]['pageing'].get('total', 0)\n",
    "                                print(f\"[INFO] 전체 문서 수: {total_docs}, 전체 페이지: {total_pages}\")\n",
    "                                \n",
    "                                if page >= total_pages:\n",
    "                                    print(f\"[INFO] 마지막 페이지 도달: {page}/{total_pages}\")\n",
    "                                    break\n",
    "                        else:\n",
    "                            print(f\"[INFO] 페이지 {page} 데이터 없음\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(f\"[WARN] 예상과 다른 응답 구조: {response.text[:200]}\")\n",
    "                        break\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"[ERROR] JSON 파싱 실패\")\n",
    "                    break\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(0.5)  # API 부하 방지\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] 페이지 {page} 처리 실패: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"[DONE] 총 {len(results)}개 문서 수집 완료\")\n",
    "        return results[:max_results]\n",
    "\n",
    "    def get_documents_by_type(self, search_gubun='s0', max_results=30000, yearfrom=None, yearto=None, lang='kor', page_size=100):\n",
    "        \"\"\"자료유형으로 문서 목록 가져오기 (b0: 단행본, s0: 연속간행물, r0: 연구보고서)\"\"\"\n",
    "        type_label_map = {'b0': '단행본', 's0': '연속간행물', 'r0': '연구보고서'}\n",
    "        type_label = type_label_map.get(search_gubun, '기타')\n",
    "        print(f\"[RUN] 자료유형 '{type_label}'({search_gubun}) 검색... (최대 {max_results}개, pageSize={page_size})\")\n",
    "\n",
    "        results = []\n",
    "        page = 1\n",
    "        per_page = page_size\n",
    "\n",
    "        total_pages = None\n",
    "        total_docs = None\n",
    "\n",
    "        while len(results) < max_results:\n",
    "            try:\n",
    "                params = {\n",
    "                    'page': page,\n",
    "                    'pageSize': per_page,\n",
    "                    'lang': lang,\n",
    "                    'search_gubun': search_gubun\n",
    "                }\n",
    "                if yearfrom:\n",
    "                    params['yearfrom'] = str(yearfrom)\n",
    "                if yearto:\n",
    "                    params['yearto'] = str(yearto)\n",
    "\n",
    "                print(f\"[RUN] 페이지 {page} 요청... (누적 {len(results)}개)\")\n",
    "                response = self.session.get(self.base_url, params=params, timeout=60)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"[WARN] HTTP 오류: {response.status_code}\")\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    if 'xmlData' in data and 'list' in data['xmlData']:\n",
    "                        list_data = data['xmlData']['list']\n",
    "                        if list_data and 'rowList' in list_data[0]:\n",
    "                            # 페이지 정보 최초 1회 기록\n",
    "                            if 'pageing' in list_data[0]:\n",
    "                                total_pages_raw = list_data[0]['pageing'].get('totalPage', 0)\n",
    "                                total_docs = list_data[0]['pageing'].get('total', 0)\n",
    "                                try:\n",
    "                                    total_pages = int(float(total_pages_raw))\n",
    "                                except Exception:\n",
    "                                    total_pages = int(total_pages_raw) if str(total_pages_raw).isdigit() else None\n",
    "                                if total_pages and total_docs:\n",
    "                                    print(f\"[INFO] 전체 문서 수: {total_docs}, 전체 페이지: {total_pages}\")\n",
    "\n",
    "                            page_results = self.parse_json_results(list_data[0]['rowList'], default_type=type_label)\n",
    "                            if not page_results:\n",
    "                                print(f\"[INFO] 페이지 {page} 더 이상 결과 없음\")\n",
    "                                break\n",
    "                            results.extend(page_results)\n",
    "                            print(f\"[OK] 페이지 {page}: {len(page_results)}개 수집 (누적 {len(results)}개)\")\n",
    "\n",
    "                            if total_pages and page >= total_pages:\n",
    "                                print(f\"[INFO] 마지막 페이지 도달: {page}/{total_pages}\")\n",
    "                                break\n",
    "\n",
    "                            if total_docs and len(results) >= int(total_docs):\n",
    "                                print(f\"[INFO] 전체 건수 {total_docs}개 도달\")\n",
    "                                break\n",
    "                        else:\n",
    "                            print(f\"[INFO] 페이지 {page} 데이터 없음\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(f\"[WARN] 예상과 다른 응답 구조: {response.text[:200]}\")\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"[ERROR] JSON 파싱 실패\")\n",
    "                    break\n",
    "\n",
    "                page += 1\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] 페이지 {page} 처리 실패: {e}\")\n",
    "                break\n",
    "\n",
    "        print(f\"[DONE] 총 {len(results)}개 문서 수집 완료\")\n",
    "        return results[:max_results]\n",
    "    \n",
    "    def parse_json_results(self, row_list, default_type=None):\n",
    "        \"\"\"JSON 응답에서 문서 정보 추출\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        type_label_map = {'b0': '단행본', 's0': '연속간행물', 'r0': '연구보고서'}\n",
    "        \n",
    "        for item in row_list:\n",
    "            try:\n",
    "                # JSON 구조에서 필요한 정보 추출\n",
    "                fact_type = item.get('factdDataTy') or item.get('factd')  # 타입 코드가 제공될 수 있음\n",
    "                type_label = type_label_map.get(fact_type, default_type) if (fact_type or default_type) else None\n",
    "                \n",
    "                result = {\n",
    "                    'title': (item.get('title') or '').strip(),\n",
    "                    'author': (item.get('author') or '').strip(),\n",
    "                    'publisher': (item.get('publisher') or '').strip(),\n",
    "                    'pubyear': item.get('pubyear', ''),\n",
    "                    'isbn': item.get('isbn', ''),\n",
    "                    'issn': item.get('issn', ''),\n",
    "                    'location': item.get('location', []),\n",
    "                    'bibctrlno': item.get('bibctrlno', ''),\n",
    "                    'detail_url': item.get('libURL', ''),\n",
    "                    'type': type_label,\n",
    "                    'source': 'KIMM Library',\n",
    "                    'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "                # 제목이 있는 경우만 추가\n",
    "                if result['title']:\n",
    "                    # 위치 정보 정리\n",
    "                    if isinstance(result['location'], list):\n",
    "                        result['location'] = ', '.join(result['location'])\n",
    "                    \n",
    "                    # 연도 정보 정리\n",
    "                    if result['pubyear']:\n",
    "                        try:\n",
    "                            result['year'] = int(result['pubyear'])\n",
    "                        except Exception:\n",
    "                            result['year'] = result['pubyear']\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] 항목 파싱 오류: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_by_query(self, query=\"\", doc_type=\"\", max_results=100):\n",
    "        \"\"\"쿼리로 문서 검색\"\"\"\n",
    "        print(f\"[RUN] 검색: '{query}' (타입: {doc_type or '전체'})\")\n",
    "        \n",
    "        results = []\n",
    "        page = 1\n",
    "        per_page = 20\n",
    "        \n",
    "        while len(results) < max_results:\n",
    "            try:\n",
    "                params = {\n",
    "                    'q': query,\n",
    "                    'page': page,\n",
    "                    'size': per_page\n",
    "                }\n",
    "                \n",
    "                if doc_type:\n",
    "                    params['type'] = doc_type\n",
    "                \n",
    "                print(f\"[RUN] 페이지 {page} 요청...\")\n",
    "                response = self.session.get(self.base_url, params=params, timeout=30)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"[WARN] HTTP 오류: {response.status_code}\")\n",
    "                    break\n",
    "                \n",
    "                # HTML 파싱 시도\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 실제 데이터 추출 (HTML 구조에 따라 조정 필요)\n",
    "                page_results = self.parse_search_results(soup)\n",
    "                \n",
    "                if not page_results:\n",
    "                    print(f\"[INFO] 페이지 {page} 더 이상 결과 없음\")\n",
    "                    break\n",
    "                \n",
    "                results.extend(page_results)\n",
    "                print(f\"[OK] 페이지 {page}: {len(page_results)}개 수집\")\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(1)  # API 과부하 방지\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] 페이지 {page} 처리 실패: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"[DONE] 총 {len(results)}개 문서 수집 완료\")\n",
    "        return results[:max_results]\n",
    "    \n",
    "    def parse_search_results(self, soup):\n",
    "        \"\"\"검색 결과 HTML에서 데이터 추출\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # HTML 구조 분석을 위한 기본 파싱\n",
    "        # 실제 사이트 구조에 따라 셀렉터 조정 필요\n",
    "        \n",
    "        # 제목과 링크 찾기\n",
    "        titles = soup.find_all(['h3', 'h4', 'a'], class_=['title', 'item-title'])\n",
    "        \n",
    "        for i, title_elem in enumerate(titles[:10]):  # 최대 10개까지\n",
    "            try:\n",
    "                title = title_elem.get_text(strip=True)\n",
    "                link = title_elem.get('href', '')\n",
    "                \n",
    "                if title and len(title) > 5:  # 유효한 제목인지 확인\n",
    "                    result = {\n",
    "                        'title': title,\n",
    "                        'link': link,\n",
    "                        'source': 'KIMM Library',\n",
    "                        'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    }\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] 항목 {i} 파싱 오류: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_sample_data(self):\n",
    "        \"\"\"API 접근이 어려운 경우 샘플 데이터 생성\"\"\"\n",
    "        print(\"[INFO] 샘플 데이터 생성...\")\n",
    "        \n",
    "        sample_data = [\n",
    "            {\n",
    "                'title': '배터리 열관리 시스템의 효율성 향상 연구',\n",
    "                'author': '김연구, 박실험',\n",
    "                'type': '연속간행물',\n",
    "                'year': 2024,\n",
    "                'journal': '한국기계학회논문집',\n",
    "                'keywords': '배터리, 열관리, SOC, 에너지효율',\n",
    "                'abstract': '리튬이온 배터리의 열관리 시스템 최적화를 통한 성능 향상 연구',\n",
    "                'institution': '한국기계연구원',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            },\n",
    "            {\n",
    "                'title': 'Modelica를 이용한 전기자동차 배터리 모델링 및 시뮬레이션',\n",
    "                'author': '이모델, 정시뮬',\n",
    "                'type': '연구보고서',\n",
    "                'year': 2024,\n",
    "                'institution': '한국기계연구원',\n",
    "                'keywords': 'Modelica, 전기자동차, 배터리모델링, 시뮬레이션',\n",
    "                'abstract': 'Modelica 언어를 활용한 전기자동차 배터리 시스템의 동적 모델링 연구',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            },\n",
    "            {\n",
    "                'title': 'SOC 추정 알고리즘의 정확도 개선에 관한 연구',\n",
    "                'author': '최추정, 한정확',\n",
    "                'type': '연속간행물',\n",
    "                'year': 2023,\n",
    "                'journal': '에너지공학회논문집',\n",
    "                'keywords': 'SOC 추정, 칼만필터, 배터리관리시스템',\n",
    "                'abstract': '확장 칼만 필터를 이용한 고정밀 SOC 추정 알고리즘 개발',\n",
    "                'institution': '한국기계연구원',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            },\n",
    "            {\n",
    "                'title': '차세대 배터리 소재 개발 동향',\n",
    "                'author': '신소재, 미래기술',\n",
    "                'type': '연구보고서',\n",
    "                'year': 2024,\n",
    "                'institution': '한국기계연구원',\n",
    "                'keywords': '차세대배터리, 소재개발, 에너지밀도',\n",
    "                'abstract': '고에너지밀도 배터리 소재 개발 현황 및 향후 전망',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            },\n",
    "            {\n",
    "                'title': '전기자동차 충전 인프라 최적화 연구',\n",
    "                'author': '전인프라, 차충전',\n",
    "                'type': '연속간행물',\n",
    "                'year': 2023,\n",
    "                'journal': '전력전자학회논문집',\n",
    "                'keywords': '전기자동차, 충전인프라, 최적화',\n",
    "                'abstract': '전기자동차 급속충전 네트워크의 효율적 배치 및 운영 방안',\n",
    "                'institution': '한국기계연구원',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"[DONE] {len(sample_data)}개 샘플 데이터 생성 완료\")\n",
    "        return sample_data\n",
    "\n",
    "print(\"[INIT] KIMMLibraryCrawler 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 크롤러 인스턴스 생성 및 API 테스트\n",
    "crawler = KIMMLibraryCrawler()\n",
    "\n",
    "# API 연결 테스트\n",
    "print(\"=\" * 50)\n",
    "print(\"🔍 KIMM 도서관 API 테스트\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "api_available = crawler.test_api_connection()\n",
    "print(f\"\\n🔗 API 사용 가능: {'✅ 예' if api_available else '❌ 아니오'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d95a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 연속간행물(논문) 수집 실행\n",
    "print(\"=\" * 50)\n",
    "print(\"📊 연속간행물(논문) 수집 시작\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# 연속간행물(s0)만 대상, 필요시 연도 범위 지정 가능\n",
    "try:\n",
    "    # page_size를 100으로 설정, 최대 30,000건까지 시도\n",
    "    all_data = crawler.get_documents_by_type(search_gubun='s0', max_results=30000, yearfrom=None, yearto=None, lang='kor', page_size=100)\n",
    "    print(f\"✅ 연속간행물 검색: {len(all_data)}개 결과\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 연속간행물 검색 실패: {e}\")\n",
    "    all_data = []\n",
    "\n",
    "# API에서 데이터를 가져오지 못한 경우 샘플 데이터 사용\n",
    "if not all_data:\n",
    "    print(\"\\n⚠️  API에서 데이터를 가져올 수 없어 샘플 데이터를 사용합니다.\")\n",
    "    all_data = crawler.create_sample_data()\n",
    "\n",
    "print(f\"\\n📋 수집된 총 데이터: {len(all_data)}개\")\n",
    "\n",
    "# 제목 기준으로 중복 제거\n",
    "seen_titles = set()\n",
    "unique_data = []\n",
    "for item in all_data:\n",
    "    title = item.get('title', '')\n",
    "    if title and title not in seen_titles:\n",
    "        seen_titles.add(title)\n",
    "        unique_data.append(item)\n",
    "\n",
    "print(f\"📋 중복 제거 후: {len(unique_data)}개\")\n",
    "\n",
    "# 데이터 미리보기\n",
    "if unique_data:\n",
    "    print(f\"\\n📝 첫 번째 항목 예시:\")\n",
    "    first_item = unique_data[0]\n",
    "    for key, value in first_item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"❌ 수집된 데이터가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 데이터프레임 생성 및 분석\n",
    "print(\"=\" * 50)\n",
    "print(\"📊 데이터 분석\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if unique_data:\n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame(unique_data)\n",
    "    \n",
    "    print(f\"📈 데이터프레임 크기: {df.shape}\")\n",
    "    print(f\"📋 컬럼: {list(df.columns)}\")\n",
    "    \n",
    "    # 기본 통계\n",
    "    print(\"\\n📊 기본 통계:\")\n",
    "    \n",
    "    # 타입별 분포\n",
    "    if 'type' in df.columns:\n",
    "        type_counts = df['type'].value_counts()\n",
    "        print(f\"📄 문서 타입별 분포:\")\n",
    "        for doc_type, count in type_counts.items():\n",
    "            print(f\"  - {doc_type}: {count}개\")\n",
    "    \n",
    "    # 연도별 분포\n",
    "    if 'year' in df.columns:\n",
    "        year_counts = df['year'].value_counts().sort_index(ascending=False)\n",
    "        print(f\"\\n📅 연도별 분포:\")\n",
    "        for year, count in year_counts.head(5).items():\n",
    "            print(f\"  - {year}: {count}개\")\n",
    "    \n",
    "    # 저자별 분포 (상위 5명)\n",
    "    if 'author' in df.columns:\n",
    "        author_counts = df['author'].value_counts()\n",
    "        print(f\"\\n👨‍🔬 주요 저자 (상위 5명):\")\n",
    "        for author, count in author_counts.head(5).items():\n",
    "            print(f\"  - {author}: {count}편\")\n",
    "    \n",
    "    # 데이터 샘플 출력\n",
    "    print(f\"\\n📋 데이터 샘플 (최대 3개):\")\n",
    "    for i, row in df.head(3).iterrows():\n",
    "        print(f\"\\n--- 항목 {i+1} ---\")\n",
    "        print(f\"제목: {row.get('title', 'N/A')}\")\n",
    "        print(f\"저자: {row.get('author', 'N/A')}\")\n",
    "        print(f\"타입: {row.get('type', 'N/A')}\")\n",
    "        print(f\"연도: {row.get('year', 'N/A')}\")\n",
    "        if 'keywords' in row:\n",
    "            print(f\"키워드: {row['keywords']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 분석할 데이터가 없습니다.\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84509cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 엑셀 파일로 저장\n",
    "print(\"=\" * 50)\n",
    "print(\"💾 엑셀 파일 저장\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not df.empty:\n",
    "    # 파일명 생성\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"KIMM_Library_Data_{timestamp}.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        # 엑셀 파일로 저장\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            # 전체 데이터\n",
    "            df.to_excel(writer, sheet_name='전체데이터', index=False)\n",
    "            \n",
    "            # 문서 타입별 시트\n",
    "            if 'type' in df.columns:\n",
    "                for doc_type in df['type'].unique():\n",
    "                    if pd.notna(doc_type):\n",
    "                        type_df = df[df['type'] == doc_type]\n",
    "                        sheet_name = str(doc_type)[:31]  # 엑셀 시트명 길이 제한\n",
    "                        type_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            \n",
    "            # 연도별 시트 (최근 5년)\n",
    "            if 'year' in df.columns:\n",
    "                recent_years = sorted(df['year'].dropna().unique(), reverse=True)[:5]\n",
    "                for year in recent_years:\n",
    "                    year_df = df[df['year'] == year]\n",
    "                    sheet_name = f\"year_{int(year)}\"\n",
    "                    year_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "        print(f\"✅ 엑셀 파일 저장 완료: {filename}\")\n",
    "        print(f\"📊 총 {len(df)}개 레코드 저장\")\n",
    "        \n",
    "        # 파일 정보 출력\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename) / 1024  # KB\n",
    "        print(f\"📏 파일 크기: {file_size:.1f} KB\")\n",
    "        print(f\"📂 저장 위치: {os.path.abspath(filename)}\")\n",
    "        \n",
    "        # 저장된 시트 정보\n",
    "        print(f\"\\n📋 저장된 시트:\")\n",
    "        print(f\"  - 전체데이터: {len(df)}개 레코드\")\n",
    "        \n",
    "        if 'type' in df.columns:\n",
    "            for doc_type in df['type'].unique():\n",
    "                if pd.notna(doc_type):\n",
    "                    count = len(df[df['type'] == doc_type])\n",
    "                    print(f\"  - {doc_type}: {count}개 레코드\")\n",
    "        \n",
    "        if 'year' in df.columns:\n",
    "            recent_years = sorted(df['year'].dropna().unique(), reverse=True)[:5]\n",
    "            for year in recent_years:\n",
    "                count = len(df[df['year'] == year])\n",
    "                print(f\"  - {int(year)}년: {count}개 레코드\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 엑셀 파일 저장 실패: {e}\")\n",
    "        \n",
    "        # CSV로 대체 저장 시도\n",
    "        try:\n",
    "            csv_filename = filename.replace('.xlsx', '.csv')\n",
    "            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"📄 CSV 파일로 대체 저장: {csv_filename}\")\n",
    "        except Exception as csv_error:\n",
    "            print(f\"❌ CSV 저장도 실패: {csv_error}\")\n",
    "            \n",
    "else:\n",
    "    print(\"❌ 저장할 데이터가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 시각화 및 결과 요약\n",
    "print(\"=\" * 50)\n",
    "print(\"📈 데이터 시각화\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not df.empty:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['font.family'] = 'Malgun Gothic'  # 한글 폰트 설정\n",
    "    matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # 서브플롯 생성\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('KIMM 도서관 데이터 분석 결과', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. 문서 타입별 분포\n",
    "    if 'type' in df.columns and len(df['type'].unique()) > 1:\n",
    "        type_counts = df['type'].value_counts()\n",
    "        axes[0, 0].pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "        axes[0, 0].set_title('문서 타입별 분포')\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, '문서 타입 데이터 없음', ha='center', va='center')\n",
    "        axes[0, 0].set_title('문서 타입별 분포')\n",
    "    \n",
    "    # 2. 연도별 분포\n",
    "    if 'year' in df.columns and df['year'].notna().sum() > 0:\n",
    "        year_counts = df['year'].value_counts().sort_index()\n",
    "        axes[0, 1].bar(year_counts.index, year_counts.values)\n",
    "        axes[0, 1].set_title('연도별 분포')\n",
    "        axes[0, 1].set_xlabel('연도')\n",
    "        axes[0, 1].set_ylabel('논문 수')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, '연도 데이터 없음', ha='center', va='center')\n",
    "        axes[0, 1].set_title('연도별 분포')\n",
    "    \n",
    "    # 3. 주요 저자\n",
    "    if 'author' in df.columns and df['author'].notna().sum() > 0:\n",
    "        author_counts = df['author'].value_counts().head(10)\n",
    "        axes[1, 0].barh(range(len(author_counts)), author_counts.values)\n",
    "        axes[1, 0].set_yticks(range(len(author_counts)))\n",
    "        axes[1, 0].set_yticklabels(author_counts.index)\n",
    "        axes[1, 0].set_title('주요 저자 (상위 10명)')\n",
    "        axes[1, 0].set_xlabel('논문 수')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, '저자 데이터 없음', ha='center', va='center')\n",
    "        axes[1, 0].set_title('주요 저자')\n",
    "    \n",
    "    # 4. 키워드 분석 (간단한 텍스트 분석)\n",
    "    if 'keywords' in df.columns and df['keywords'].notna().sum() > 0:\n",
    "        # 키워드 추출 및 빈도 계산\n",
    "        all_keywords = []\n",
    "        for keywords in df['keywords'].dropna():\n",
    "            if isinstance(keywords, str):\n",
    "                words = [word.strip() for word in keywords.split(',')]\n",
    "                all_keywords.extend(words)\n",
    "        \n",
    "        if all_keywords:\n",
    "            from collections import Counter\n",
    "            keyword_counts = Counter(all_keywords)\n",
    "            top_keywords = dict(keyword_counts.most_common(10))\n",
    "            \n",
    "            axes[1, 1].barh(range(len(top_keywords)), list(top_keywords.values()))\n",
    "            axes[1, 1].set_yticks(range(len(top_keywords)))\n",
    "            axes[1, 1].set_yticklabels(list(top_keywords.keys()))\n",
    "            axes[1, 1].set_title('주요 키워드 (상위 10개)')\n",
    "            axes[1, 1].set_xlabel('빈도')\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, '키워드 분석 불가', ha='center', va='center')\n",
    "            axes[1, 1].set_title('주요 키워드')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, '키워드 데이터 없음', ha='center', va='center')\n",
    "        axes[1, 1].set_title('주요 키워드')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 시각화할 데이터가 없습니다.\")\n",
    "\n",
    "# 최종 결과 요약\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🎉 크롤링 작업 완료!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"✅ 총 수집 문서 수: {len(df)}개\")\n",
    "    print(f\"📂 저장 파일: {filename if 'filename' in locals() else 'N/A'}\")\n",
    "    print(f\"🕐 작업 완료 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if 'type' in df.columns:\n",
    "        print(f\"\\n📊 문서 타입별 요약:\")\n",
    "        for doc_type, count in df['type'].value_counts().items():\n",
    "            print(f\"  - {doc_type}: {count}개\")\n",
    "else:\n",
    "    print(\"❌ 수집된 데이터가 없습니다.\")\n",
    "    print(\"💡 API 접근 문제나 네트워크 이슈일 수 있습니다.\")\n",
    "\n",
    "print(f\"\\n📝 작업 로그:\")\n",
    "print(f\"  - API 테스트: {'성공' if 'api_available' in locals() and api_available else '실패'}\")\n",
    "print(f\"  - 데이터 수집: {'성공' if 'unique_data' in locals() and unique_data else '실패'}\")\n",
    "print(f\"  - 엑셀 저장: {'성공' if 'filename' in locals() else '실패'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08f1a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주요 키워드 (빈도수 포함):\n",
      "laser: 153\n",
      "high: 123\n",
      "hydrogen: 85\n",
      "plasma: 81\n",
      "air: 77\n",
      "via: 75\n",
      "heat: 72\n",
      "thermal: 70\n",
      "gas: 70\n",
      "characteristics: 69\n",
      "machine: 63\n",
      "micro: 55\n",
      "temperature: 52\n",
      "pump: 52\n",
      "energy: 52\n",
      "roll: 51\n",
      "technology: 50\n",
      "ion: 49\n",
      "control: 49\n",
      "따른: 48\n",
      "개발development: 47\n",
      "fuel: 46\n",
      "sensor: 45\n",
      "measurement: 45\n",
      "transfer: 45\n",
      "liquid: 45\n",
      "manufacturing: 44\n",
      "연구a: 43\n",
      "3d: 43\n",
      "film: 43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "\n",
    "# path to csv\n",
    "csv_path = r'D:\\simu\\TB\\Modelica_Modelon_KETI\\TwinBuilder_Modelon\\KIMM_rt_20250829_174129_y2023_2025.csv'\n",
    "df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "\n",
    "# 2. 모든 논문 제목을 하나의 문자열로 결합합니다.\n",
    "text = ' '.join(df['title'].astype(str))\n",
    "\n",
    "# 3. 한글 및 영문 단어를 추출하기 위한 정규표현식 (숫자 포함한 연속 문자열)\n",
    "pattern = re.compile(r\"[A-Za-z0-9\\uac00-\\ud7a3]+\")\n",
    "tokens = pattern.findall(text)\n",
    "\n",
    "# 4. 불용어 목록: 조사, 일반 동사, 평가/분석/모델 등 연구 주제와 직접 관련 없는 단어를 정의합니다.\n",
    "stopwords = {\n",
    "    'and','the','for','with','using','use','in','of','to','a','on','as','by','an',\n",
    "    'at','from','this','that','is','are','be','into','based','study','analysis',\n",
    "    'model','design','development','evaluation','case','report','experiment','properties',\n",
    "    '연구','기술','개발','모델','분석','평가','성능','특성',\n",
    "    '이용한','활용한','이용','활용','사용한','사용','위한','관한','기반','관련',\n",
    "    '테스트','시험','도입','제조','제작','방법','방법론','실험','예측','측정',\n",
    "    'process','performance','system','method','methods','effect','effects','result','results',\n",
    "    # 한글 조사\n",
    "    '의','을','를','과','및','에','는','가','도','로','에서','등','중','한','것','이다'\n",
    "}\n",
    "\n",
    "# 5. 토큰을 필터링합니다: 숫자만 또는 길이가 1인 토큰, 불용어를 제외합니다.\n",
    "def is_valid(tok):\n",
    "    return len(tok) > 1 and not tok.isdigit()\n",
    "\n",
    "filtered_tokens = [\n",
    "    tok.lower() for tok in tokens\n",
    "    if is_valid(tok) and tok.lower() not in stopwords\n",
    "]\n",
    "\n",
    "# 6. 빈도수 계산 후 상위 키워드 추출\n",
    "counter = Counter(filtered_tokens)\n",
    "top_keywords = counter.most_common(30)  # 상위 30개 키워드 출력\n",
    "\n",
    "# 7. 결과 출력\n",
    "print(\"주요 키워드 (빈도수 포함):\")\n",
    "for word, freq in top_keywords:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
