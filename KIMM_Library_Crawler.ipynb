{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb5d651",
   "metadata": {},
   "source": [
    "# KIMM ë„ì„œê´€ API í¬ë¡¤ëŸ¬\n",
    "## í•œêµ­ê¸°ê³„ì—°êµ¬ì› ì—°êµ¬ë…¼ë¬¸ ë° ì—°êµ¬ë³´ê³ ì„œ ìˆ˜ì§‘\n",
    "\n",
    "**ëª©í‘œ**: https://library.kimm.re.kr/openAPI/openAPI_allsearch.do APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬ë…¼ë¬¸ê³¼ ì—°êµ¬ë³´ê³ ì„œ ëª©ë¡ì„ í¬ë¡¤ë§í•˜ê³  ì—‘ì…€ë¡œ ì¶”ì¶œ\n",
    "\n",
    "---\n",
    "*ì‘ì„±ì¼: 2025ë…„ 8ì›” 29ì¼*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"âœ… {package} ì´ë¯¸ ì„¤ì¹˜ë¨\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "\n",
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ ì„¤ì¹˜\n",
    "packages = ['requests', 'pandas', 'openpyxl', 'beautifulsoup4', 'lxml']\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6801db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. KIMM ë„ì„œê´€ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì •ì˜\n",
    "class KIMMLibraryCrawler:\n",
    "    \"\"\"í•œêµ­ê¸°ê³„ì—°êµ¬ì› ë„ì„œê´€ API í¬ë¡¤ëŸ¬\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://library.kimm.re.kr/openAPI/openAPI_allsearch.do\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.data = []\n",
    "        print(\"[INIT] KIMM ë„ì„œê´€ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        \n",
    "    def test_api_connection(self):\n",
    "        \"\"\"API ì—°ê²° í…ŒìŠ¤íŠ¸\"\"\"\n",
    "        print(\"[TEST] API ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "        try:\n",
    "            response = self.session.get(self.base_url, timeout=10)\n",
    "            print(f\"[TEST] ì‘ë‹µ ìƒíƒœ: {response.status_code}\")\n",
    "            print(f\"[TEST] ì‘ë‹µ íƒ€ì…: {response.headers.get('content-type', 'Unknown')}\")\n",
    "            print(f\"[TEST] ì‘ë‹µ í¬ê¸°: {len(response.text)} bytes\")\n",
    "            \n",
    "            # ì‘ë‹µ ë‚´ìš© ì¼ë¶€ ì¶œë ¥\n",
    "            if response.text:\n",
    "                print(\"[TEST] ì‘ë‹µ ë‚´ìš© (ì²« 500ì):\")\n",
    "                print(response.text[:500])\n",
    "            \n",
    "            return response.status_code == 200\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] API ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_all_documents(self, max_results=5000):\n",
    "        \"\"\"ì „ì²´ ë¬¸ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (í‚¤ì›Œë“œ ì—†ì´)\"\"\"\n",
    "        print(f\"[RUN] ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰... (ìµœëŒ€ {max_results}ê°œ)\")\n",
    "        \n",
    "        results = []\n",
    "        page = 1\n",
    "        per_page = 50  # í•œ ë²ˆì— ë” ë§ì´ ê°€ì ¸ì˜¤ê¸°\n",
    "        \n",
    "        while len(results) < max_results:\n",
    "            try:\n",
    "                # ë¹ˆ ì¿¼ë¦¬ë¡œ ì „ì²´ ê²€ìƒ‰\n",
    "                params = {\n",
    "                    'page': page,\n",
    "                    'pageSize': per_page,\n",
    "                    'lang': 'kor'\n",
    "                }\n",
    "                \n",
    "                print(f\"[RUN] í˜ì´ì§€ {page} ìš”ì²­... (ëˆ„ì  {len(results)}ê°œ)\")\n",
    "                response = self.session.get(self.base_url, params=params, timeout=30)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"[WARN] HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    break\n",
    "                \n",
    "                # JSON ì‘ë‹µ íŒŒì‹±\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    if 'xmlData' in data and 'list' in data['xmlData']:\n",
    "                        list_data = data['xmlData']['list']\n",
    "                        if list_data and 'rowList' in list_data[0]:\n",
    "                            page_results = self.parse_json_results(list_data[0]['rowList'])\n",
    "                            \n",
    "                            if not page_results:\n",
    "                                print(f\"[INFO] í˜ì´ì§€ {page} ë” ì´ìƒ ê²°ê³¼ ì—†ìŒ\")\n",
    "                                break\n",
    "                            \n",
    "                            results.extend(page_results)\n",
    "                            print(f\"[OK] í˜ì´ì§€ {page}: {len(page_results)}ê°œ ìˆ˜ì§‘\")\n",
    "                            \n",
    "                            # ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸\n",
    "                            if 'pageing' in list_data[0]:\n",
    "                                total_pages_raw = list_data[0]['pageing'].get('totalPage', 0)\n",
    "                                try:\n",
    "                                    total_pages = int(float(total_pages_raw))\n",
    "                                except Exception:\n",
    "                                    total_pages = int(total_pages_raw) if str(total_pages_raw).isdigit() else 0\n",
    "                                total_docs = list_data[0]['pageing'].get('total', 0)\n",
    "                                print(f\"[INFO] ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_docs}, ì „ì²´ í˜ì´ì§€: {total_pages}\")\n",
    "                                \n",
    "                                if page >= total_pages:\n",
    "                                    print(f\"[INFO] ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬: {page}/{total_pages}\")\n",
    "                                    break\n",
    "                        else:\n",
    "                            print(f\"[INFO] í˜ì´ì§€ {page} ë°ì´í„° ì—†ìŒ\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(f\"[WARN] ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì‘ë‹µ êµ¬ì¡°: {response.text[:200]}\")\n",
    "                        break\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"[ERROR] JSON íŒŒì‹± ì‹¤íŒ¨\")\n",
    "                    break\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(0.5)  # API ë¶€í•˜ ë°©ì§€\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"[DONE] ì´ {len(results)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "        return results[:max_results]\n",
    "\n",
    "    def get_documents_by_type(self, search_gubun='s0', max_results=30000, yearfrom=None, yearto=None, lang='kor', page_size=100):\n",
    "        \"\"\"ìë£Œìœ í˜•ìœ¼ë¡œ ë¬¸ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (b0: ë‹¨í–‰ë³¸, s0: ì—°ì†ê°„í–‰ë¬¼, r0: ì—°êµ¬ë³´ê³ ì„œ)\"\"\"\n",
    "        type_label_map = {'b0': 'ë‹¨í–‰ë³¸', 's0': 'ì—°ì†ê°„í–‰ë¬¼', 'r0': 'ì—°êµ¬ë³´ê³ ì„œ'}\n",
    "        type_label = type_label_map.get(search_gubun, 'ê¸°íƒ€')\n",
    "        print(f\"[RUN] ìë£Œìœ í˜• '{type_label}'({search_gubun}) ê²€ìƒ‰... (ìµœëŒ€ {max_results}ê°œ, pageSize={page_size})\")\n",
    "\n",
    "        results = []\n",
    "        page = 1\n",
    "        per_page = page_size\n",
    "\n",
    "        total_pages = None\n",
    "        total_docs = None\n",
    "\n",
    "        while len(results) < max_results:\n",
    "            try:\n",
    "                params = {\n",
    "                    'page': page,\n",
    "                    'pageSize': per_page,\n",
    "                    'lang': lang,\n",
    "                    'search_gubun': search_gubun\n",
    "                }\n",
    "                if yearfrom:\n",
    "                    params['yearfrom'] = str(yearfrom)\n",
    "                if yearto:\n",
    "                    params['yearto'] = str(yearto)\n",
    "\n",
    "                print(f\"[RUN] í˜ì´ì§€ {page} ìš”ì²­... (ëˆ„ì  {len(results)}ê°œ)\")\n",
    "                response = self.session.get(self.base_url, params=params, timeout=60)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"[WARN] HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    if 'xmlData' in data and 'list' in data['xmlData']:\n",
    "                        list_data = data['xmlData']['list']\n",
    "                        if list_data and 'rowList' in list_data[0]:\n",
    "                            # í˜ì´ì§€ ì •ë³´ ìµœì´ˆ 1íšŒ ê¸°ë¡\n",
    "                            if 'pageing' in list_data[0]:\n",
    "                                total_pages_raw = list_data[0]['pageing'].get('totalPage', 0)\n",
    "                                total_docs = list_data[0]['pageing'].get('total', 0)\n",
    "                                try:\n",
    "                                    total_pages = int(float(total_pages_raw))\n",
    "                                except Exception:\n",
    "                                    total_pages = int(total_pages_raw) if str(total_pages_raw).isdigit() else None\n",
    "                                if total_pages and total_docs:\n",
    "                                    print(f\"[INFO] ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_docs}, ì „ì²´ í˜ì´ì§€: {total_pages}\")\n",
    "\n",
    "                            page_results = self.parse_json_results(list_data[0]['rowList'], default_type=type_label)\n",
    "                            if not page_results:\n",
    "                                print(f\"[INFO] í˜ì´ì§€ {page} ë” ì´ìƒ ê²°ê³¼ ì—†ìŒ\")\n",
    "                                break\n",
    "                            results.extend(page_results)\n",
    "                            print(f\"[OK] í˜ì´ì§€ {page}: {len(page_results)}ê°œ ìˆ˜ì§‘ (ëˆ„ì  {len(results)}ê°œ)\")\n",
    "\n",
    "                            if total_pages and page >= total_pages:\n",
    "                                print(f\"[INFO] ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬: {page}/{total_pages}\")\n",
    "                                break\n",
    "\n",
    "                            if total_docs and len(results) >= int(total_docs):\n",
    "                                print(f\"[INFO] ì „ì²´ ê±´ìˆ˜ {total_docs}ê°œ ë„ë‹¬\")\n",
    "                                break\n",
    "                        else:\n",
    "                            print(f\"[INFO] í˜ì´ì§€ {page} ë°ì´í„° ì—†ìŒ\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(f\"[WARN] ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì‘ë‹µ êµ¬ì¡°: {response.text[:200]}\")\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"[ERROR] JSON íŒŒì‹± ì‹¤íŒ¨\")\n",
    "                    break\n",
    "\n",
    "                page += 1\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "                break\n",
    "\n",
    "        print(f\"[DONE] ì´ {len(results)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "        return results[:max_results]\n",
    "    \n",
    "    def parse_json_results(self, row_list, default_type=None):\n",
    "        \"\"\"JSON ì‘ë‹µì—ì„œ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        type_label_map = {'b0': 'ë‹¨í–‰ë³¸', 's0': 'ì—°ì†ê°„í–‰ë¬¼', 'r0': 'ì—°êµ¬ë³´ê³ ì„œ'}\n",
    "        \n",
    "        for item in row_list:\n",
    "            try:\n",
    "                # JSON êµ¬ì¡°ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ\n",
    "                fact_type = item.get('factdDataTy') or item.get('factd')  # íƒ€ì… ì½”ë“œê°€ ì œê³µë  ìˆ˜ ìˆìŒ\n",
    "                type_label = type_label_map.get(fact_type, default_type) if (fact_type or default_type) else None\n",
    "                \n",
    "                result = {\n",
    "                    'title': (item.get('title') or '').strip(),\n",
    "                    'author': (item.get('author') or '').strip(),\n",
    "                    'publisher': (item.get('publisher') or '').strip(),\n",
    "                    'pubyear': item.get('pubyear', ''),\n",
    "                    'isbn': item.get('isbn', ''),\n",
    "                    'issn': item.get('issn', ''),\n",
    "                    'location': item.get('location', []),\n",
    "                    'bibctrlno': item.get('bibctrlno', ''),\n",
    "                    'detail_url': item.get('libURL', ''),\n",
    "                    'type': type_label,\n",
    "                    'source': 'KIMM Library',\n",
    "                    'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "                # ì œëª©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€\n",
    "                if result['title']:\n",
    "                    # ìœ„ì¹˜ ì •ë³´ ì •ë¦¬\n",
    "                    if isinstance(result['location'], list):\n",
    "                        result['location'] = ', '.join(result['location'])\n",
    "                    \n",
    "                    # ì—°ë„ ì •ë³´ ì •ë¦¬\n",
    "                    if result['pubyear']:\n",
    "                        try:\n",
    "                            result['year'] = int(result['pubyear'])\n",
    "                        except Exception:\n",
    "                            result['year'] = result['pubyear']\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_by_query(self, query=\"\", doc_type=\"\", max_results=100):\n",
    "        \"\"\"ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰\"\"\"\n",
    "        print(f\"[RUN] ê²€ìƒ‰: '{query}' (íƒ€ì…: {doc_type or 'ì „ì²´'})\")\n",
    "        \n",
    "        results = []\n",
    "        page = 1\n",
    "        per_page = 20\n",
    "        \n",
    "        while len(results) < max_results:\n",
    "            try:\n",
    "                params = {\n",
    "                    'q': query,\n",
    "                    'page': page,\n",
    "                    'size': per_page\n",
    "                }\n",
    "                \n",
    "                if doc_type:\n",
    "                    params['type'] = doc_type\n",
    "                \n",
    "                print(f\"[RUN] í˜ì´ì§€ {page} ìš”ì²­...\")\n",
    "                response = self.session.get(self.base_url, params=params, timeout=30)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"[WARN] HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    break\n",
    "                \n",
    "                # HTML íŒŒì‹± ì‹œë„\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ (HTML êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)\n",
    "                page_results = self.parse_search_results(soup)\n",
    "                \n",
    "                if not page_results:\n",
    "                    print(f\"[INFO] í˜ì´ì§€ {page} ë” ì´ìƒ ê²°ê³¼ ì—†ìŒ\")\n",
    "                    break\n",
    "                \n",
    "                results.extend(page_results)\n",
    "                print(f\"[OK] í˜ì´ì§€ {page}: {len(page_results)}ê°œ ìˆ˜ì§‘\")\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(1)  # API ê³¼ë¶€í•˜ ë°©ì§€\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"[DONE] ì´ {len(results)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "        return results[:max_results]\n",
    "    \n",
    "    def parse_search_results(self, soup):\n",
    "        \"\"\"ê²€ìƒ‰ ê²°ê³¼ HTMLì—ì„œ ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # HTML êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ê¸°ë³¸ íŒŒì‹±\n",
    "        # ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ì…€ë ‰í„° ì¡°ì • í•„ìš”\n",
    "        \n",
    "        # ì œëª©ê³¼ ë§í¬ ì°¾ê¸°\n",
    "        titles = soup.find_all(['h3', 'h4', 'a'], class_=['title', 'item-title'])\n",
    "        \n",
    "        for i, title_elem in enumerate(titles[:10]):  # ìµœëŒ€ 10ê°œê¹Œì§€\n",
    "            try:\n",
    "                title = title_elem.get_text(strip=True)\n",
    "                link = title_elem.get('href', '')\n",
    "                \n",
    "                if title and len(title) > 5:  # ìœ íš¨í•œ ì œëª©ì¸ì§€ í™•ì¸\n",
    "                    result = {\n",
    "                        'title': title,\n",
    "                        'link': link,\n",
    "                        'source': 'KIMM Library',\n",
    "                        'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    }\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] í•­ëª© {i} íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_sample_data(self):\n",
    "        \"\"\"API ì ‘ê·¼ì´ ì–´ë ¤ìš´ ê²½ìš° ìƒ˜í”Œ ë°ì´í„° ìƒì„±\"\"\"\n",
    "        print(\"[INFO] ìƒ˜í”Œ ë°ì´í„° ìƒì„±...\")\n",
    "        \n",
    "        sample_data = [\n",
    "            {\n",
    "                'title': 'ë°°í„°ë¦¬ ì—´ê´€ë¦¬ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„± í–¥ìƒ ì—°êµ¬',\n",
    "                'author': 'ê¹€ì—°êµ¬, ë°•ì‹¤í—˜',\n",
    "                'type': 'ì—°ì†ê°„í–‰ë¬¼',\n",
    "                'year': 2024,\n",
    "                'journal': 'í•œêµ­ê¸°ê³„í•™íšŒë…¼ë¬¸ì§‘',\n",
    "                'keywords': 'ë°°í„°ë¦¬, ì—´ê´€ë¦¬, SOC, ì—ë„ˆì§€íš¨ìœ¨',\n",
    "                'abstract': 'ë¦¬íŠ¬ì´ì˜¨ ë°°í„°ë¦¬ì˜ ì—´ê´€ë¦¬ ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ í†µí•œ ì„±ëŠ¥ í–¥ìƒ ì—°êµ¬',\n",
    "                'institution': 'í•œêµ­ê¸°ê³„ì—°êµ¬ì›',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            },\n",
    "            {\n",
    "                'title': 'Modelicaë¥¼ ì´ìš©í•œ ì „ê¸°ìë™ì°¨ ë°°í„°ë¦¬ ëª¨ë¸ë§ ë° ì‹œë®¬ë ˆì´ì…˜',\n",
    "                'author': 'ì´ëª¨ë¸, ì •ì‹œë®¬',\n",
    "                'type': 'ì—°êµ¬ë³´ê³ ì„œ',\n",
    "                'year': 2024,\n",
    "                'institution': 'í•œêµ­ê¸°ê³„ì—°êµ¬ì›',\n",
    "                'keywords': 'Modelica, ì „ê¸°ìë™ì°¨, ë°°í„°ë¦¬ëª¨ë¸ë§, ì‹œë®¬ë ˆì´ì…˜',\n",
    "                'abstract': 'Modelica ì–¸ì–´ë¥¼ í™œìš©í•œ ì „ê¸°ìë™ì°¨ ë°°í„°ë¦¬ ì‹œìŠ¤í…œì˜ ë™ì  ëª¨ë¸ë§ ì—°êµ¬',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            },\n",
    "            {\n",
    "                'title': 'SOC ì¶”ì • ì•Œê³ ë¦¬ì¦˜ì˜ ì •í™•ë„ ê°œì„ ì— ê´€í•œ ì—°êµ¬',\n",
    "                'author': 'ìµœì¶”ì •, í•œì •í™•',\n",
    "                'type': 'ì—°ì†ê°„í–‰ë¬¼',\n",
    "                'year': 2023,\n",
    "                'journal': 'ì—ë„ˆì§€ê³µí•™íšŒë…¼ë¬¸ì§‘',\n",
    "                'keywords': 'SOC ì¶”ì •, ì¹¼ë§Œí•„í„°, ë°°í„°ë¦¬ê´€ë¦¬ì‹œìŠ¤í…œ',\n",
    "                'abstract': 'í™•ì¥ ì¹¼ë§Œ í•„í„°ë¥¼ ì´ìš©í•œ ê³ ì •ë°€ SOC ì¶”ì • ì•Œê³ ë¦¬ì¦˜ ê°œë°œ',\n",
    "                'institution': 'í•œêµ­ê¸°ê³„ì—°êµ¬ì›',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            },\n",
    "            {\n",
    "                'title': 'ì°¨ì„¸ëŒ€ ë°°í„°ë¦¬ ì†Œì¬ ê°œë°œ ë™í–¥',\n",
    "                'author': 'ì‹ ì†Œì¬, ë¯¸ë˜ê¸°ìˆ ',\n",
    "                'type': 'ì—°êµ¬ë³´ê³ ì„œ',\n",
    "                'year': 2024,\n",
    "                'institution': 'í•œêµ­ê¸°ê³„ì—°êµ¬ì›',\n",
    "                'keywords': 'ì°¨ì„¸ëŒ€ë°°í„°ë¦¬, ì†Œì¬ê°œë°œ, ì—ë„ˆì§€ë°€ë„',\n",
    "                'abstract': 'ê³ ì—ë„ˆì§€ë°€ë„ ë°°í„°ë¦¬ ì†Œì¬ ê°œë°œ í˜„í™© ë° í–¥í›„ ì „ë§',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            },\n",
    "            {\n",
    "                'title': 'ì „ê¸°ìë™ì°¨ ì¶©ì „ ì¸í”„ë¼ ìµœì í™” ì—°êµ¬',\n",
    "                'author': 'ì „ì¸í”„ë¼, ì°¨ì¶©ì „',\n",
    "                'type': 'ì—°ì†ê°„í–‰ë¬¼',\n",
    "                'year': 2023,\n",
    "                'journal': 'ì „ë ¥ì „ìí•™íšŒë…¼ë¬¸ì§‘',\n",
    "                'keywords': 'ì „ê¸°ìë™ì°¨, ì¶©ì „ì¸í”„ë¼, ìµœì í™”',\n",
    "                'abstract': 'ì „ê¸°ìë™ì°¨ ê¸‰ì†ì¶©ì „ ë„¤íŠ¸ì›Œí¬ì˜ íš¨ìœ¨ì  ë°°ì¹˜ ë° ìš´ì˜ ë°©ì•ˆ',\n",
    "                'institution': 'í•œêµ­ê¸°ê³„ì—°êµ¬ì›',\n",
    "                'source': 'KIMM Library (Sample)',\n",
    "                'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"[DONE] {len(sample_data)}ê°œ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ\")\n",
    "        return sample_data\n",
    "\n",
    "print(\"[INIT] KIMMLibraryCrawler í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° API í…ŒìŠ¤íŠ¸\n",
    "crawler = KIMMLibraryCrawler()\n",
    "\n",
    "# API ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ” KIMM ë„ì„œê´€ API í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "api_available = crawler.test_api_connection()\n",
    "print(f\"\\nğŸ”— API ì‚¬ìš© ê°€ëŠ¥: {'âœ… ì˜ˆ' if api_available else 'âŒ ì•„ë‹ˆì˜¤'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d95a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ì—°ì†ê°„í–‰ë¬¼(ë…¼ë¬¸) ìˆ˜ì§‘ ì‹¤í–‰\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“Š ì—°ì†ê°„í–‰ë¬¼(ë…¼ë¬¸) ìˆ˜ì§‘ ì‹œì‘\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# ì—°ì†ê°„í–‰ë¬¼(s0)ë§Œ ëŒ€ìƒ, í•„ìš”ì‹œ ì—°ë„ ë²”ìœ„ ì§€ì • ê°€ëŠ¥\n",
    "try:\n",
    "    # page_sizeë¥¼ 100ìœ¼ë¡œ ì„¤ì •, ìµœëŒ€ 30,000ê±´ê¹Œì§€ ì‹œë„\n",
    "    all_data = crawler.get_documents_by_type(search_gubun='s0', max_results=30000, yearfrom=None, yearto=None, lang='kor', page_size=100)\n",
    "    print(f\"âœ… ì—°ì†ê°„í–‰ë¬¼ ê²€ìƒ‰: {len(all_data)}ê°œ ê²°ê³¼\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì—°ì†ê°„í–‰ë¬¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    all_data = []\n",
    "\n",
    "# APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê²½ìš° ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©\n",
    "if not all_data:\n",
    "    print(\"\\nâš ï¸  APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    all_data = crawler.create_sample_data()\n",
    "\n",
    "print(f\"\\nğŸ“‹ ìˆ˜ì§‘ëœ ì´ ë°ì´í„°: {len(all_data)}ê°œ\")\n",
    "\n",
    "# ì œëª© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "seen_titles = set()\n",
    "unique_data = []\n",
    "for item in all_data:\n",
    "    title = item.get('title', '')\n",
    "    if title and title not in seen_titles:\n",
    "        seen_titles.add(title)\n",
    "        unique_data.append(item)\n",
    "\n",
    "print(f\"ğŸ“‹ ì¤‘ë³µ ì œê±° í›„: {len(unique_data)}ê°œ\")\n",
    "\n",
    "# ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "if unique_data:\n",
    "    print(f\"\\nğŸ“ ì²« ë²ˆì§¸ í•­ëª© ì˜ˆì‹œ:\")\n",
    "    first_item = unique_data[0]\n",
    "    for key, value in first_item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ë¶„ì„\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“Š ë°ì´í„° ë¶„ì„\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if unique_data:\n",
    "    # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    df = pd.DataFrame(unique_data)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {df.shape}\")\n",
    "    print(f\"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}\")\n",
    "    \n",
    "    # ê¸°ë³¸ í†µê³„\n",
    "    print(\"\\nğŸ“Š ê¸°ë³¸ í†µê³„:\")\n",
    "    \n",
    "    # íƒ€ì…ë³„ ë¶„í¬\n",
    "    if 'type' in df.columns:\n",
    "        type_counts = df['type'].value_counts()\n",
    "        print(f\"ğŸ“„ ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬:\")\n",
    "        for doc_type, count in type_counts.items():\n",
    "            print(f\"  - {doc_type}: {count}ê°œ\")\n",
    "    \n",
    "    # ì—°ë„ë³„ ë¶„í¬\n",
    "    if 'year' in df.columns:\n",
    "        year_counts = df['year'].value_counts().sort_index(ascending=False)\n",
    "        print(f\"\\nğŸ“… ì—°ë„ë³„ ë¶„í¬:\")\n",
    "        for year, count in year_counts.head(5).items():\n",
    "            print(f\"  - {year}: {count}ê°œ\")\n",
    "    \n",
    "    # ì €ìë³„ ë¶„í¬ (ìƒìœ„ 5ëª…)\n",
    "    if 'author' in df.columns:\n",
    "        author_counts = df['author'].value_counts()\n",
    "        print(f\"\\nğŸ‘¨â€ğŸ”¬ ì£¼ìš” ì €ì (ìƒìœ„ 5ëª…):\")\n",
    "        for author, count in author_counts.head(5).items():\n",
    "            print(f\"  - {author}: {count}í¸\")\n",
    "    \n",
    "    # ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥\n",
    "    print(f\"\\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ (ìµœëŒ€ 3ê°œ):\")\n",
    "    for i, row in df.head(3).iterrows():\n",
    "        print(f\"\\n--- í•­ëª© {i+1} ---\")\n",
    "        print(f\"ì œëª©: {row.get('title', 'N/A')}\")\n",
    "        print(f\"ì €ì: {row.get('author', 'N/A')}\")\n",
    "        print(f\"íƒ€ì…: {row.get('type', 'N/A')}\")\n",
    "        print(f\"ì—°ë„: {row.get('year', 'N/A')}\")\n",
    "        if 'keywords' in row:\n",
    "            print(f\"í‚¤ì›Œë“œ: {row['keywords']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84509cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ’¾ ì—‘ì…€ íŒŒì¼ ì €ì¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not df.empty:\n",
    "    # íŒŒì¼ëª… ìƒì„±\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"KIMM_Library_Data_{timestamp}.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            # ì „ì²´ ë°ì´í„°\n",
    "            df.to_excel(writer, sheet_name='ì „ì²´ë°ì´í„°', index=False)\n",
    "            \n",
    "            # ë¬¸ì„œ íƒ€ì…ë³„ ì‹œíŠ¸\n",
    "            if 'type' in df.columns:\n",
    "                for doc_type in df['type'].unique():\n",
    "                    if pd.notna(doc_type):\n",
    "                        type_df = df[df['type'] == doc_type]\n",
    "                        sheet_name = str(doc_type)[:31]  # ì—‘ì…€ ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ\n",
    "                        type_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            \n",
    "            # ì—°ë„ë³„ ì‹œíŠ¸ (ìµœê·¼ 5ë…„)\n",
    "            if 'year' in df.columns:\n",
    "                recent_years = sorted(df['year'].dropna().unique(), reverse=True)[:5]\n",
    "                for year in recent_years:\n",
    "                    year_df = df[df['year'] == year]\n",
    "                    sheet_name = f\"year_{int(year)}\"\n",
    "                    year_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "        print(f\"âœ… ì—‘ì…€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "        print(f\"ğŸ“Š ì´ {len(df)}ê°œ ë ˆì½”ë“œ ì €ì¥\")\n",
    "        \n",
    "        # íŒŒì¼ ì •ë³´ ì¶œë ¥\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename) / 1024  # KB\n",
    "        print(f\"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:.1f} KB\")\n",
    "        print(f\"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(filename)}\")\n",
    "        \n",
    "        # ì €ì¥ëœ ì‹œíŠ¸ ì •ë³´\n",
    "        print(f\"\\nğŸ“‹ ì €ì¥ëœ ì‹œíŠ¸:\")\n",
    "        print(f\"  - ì „ì²´ë°ì´í„°: {len(df)}ê°œ ë ˆì½”ë“œ\")\n",
    "        \n",
    "        if 'type' in df.columns:\n",
    "            for doc_type in df['type'].unique():\n",
    "                if pd.notna(doc_type):\n",
    "                    count = len(df[df['type'] == doc_type])\n",
    "                    print(f\"  - {doc_type}: {count}ê°œ ë ˆì½”ë“œ\")\n",
    "        \n",
    "        if 'year' in df.columns:\n",
    "            recent_years = sorted(df['year'].dropna().unique(), reverse=True)[:5]\n",
    "            for year in recent_years:\n",
    "                count = len(df[df['year'] == year])\n",
    "                print(f\"  - {int(year)}ë…„: {count}ê°œ ë ˆì½”ë“œ\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì—‘ì…€ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        # CSVë¡œ ëŒ€ì²´ ì €ì¥ ì‹œë„\n",
    "        try:\n",
    "            csv_filename = filename.replace('.xlsx', '.csv')\n",
    "            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"ğŸ“„ CSV íŒŒì¼ë¡œ ëŒ€ì²´ ì €ì¥: {csv_filename}\")\n",
    "        except Exception as csv_error:\n",
    "            print(f\"âŒ CSV ì €ì¥ë„ ì‹¤íŒ¨: {csv_error}\")\n",
    "            \n",
    "else:\n",
    "    print(\"âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. ì‹œê°í™” ë° ê²°ê³¼ ìš”ì•½\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“ˆ ë°ì´í„° ì‹œê°í™”\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not df.empty:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['font.family'] = 'Malgun Gothic'  # í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "    matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # ì„œë¸Œí”Œë¡¯ ìƒì„±\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('KIMM ë„ì„œê´€ ë°ì´í„° ë¶„ì„ ê²°ê³¼', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬\n",
    "    if 'type' in df.columns and len(df['type'].unique()) > 1:\n",
    "        type_counts = df['type'].value_counts()\n",
    "        axes[0, 0].pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "        axes[0, 0].set_title('ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬')\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'ë¬¸ì„œ íƒ€ì… ë°ì´í„° ì—†ìŒ', ha='center', va='center')\n",
    "        axes[0, 0].set_title('ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬')\n",
    "    \n",
    "    # 2. ì—°ë„ë³„ ë¶„í¬\n",
    "    if 'year' in df.columns and df['year'].notna().sum() > 0:\n",
    "        year_counts = df['year'].value_counts().sort_index()\n",
    "        axes[0, 1].bar(year_counts.index, year_counts.values)\n",
    "        axes[0, 1].set_title('ì—°ë„ë³„ ë¶„í¬')\n",
    "        axes[0, 1].set_xlabel('ì—°ë„')\n",
    "        axes[0, 1].set_ylabel('ë…¼ë¬¸ ìˆ˜')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'ì—°ë„ ë°ì´í„° ì—†ìŒ', ha='center', va='center')\n",
    "        axes[0, 1].set_title('ì—°ë„ë³„ ë¶„í¬')\n",
    "    \n",
    "    # 3. ì£¼ìš” ì €ì\n",
    "    if 'author' in df.columns and df['author'].notna().sum() > 0:\n",
    "        author_counts = df['author'].value_counts().head(10)\n",
    "        axes[1, 0].barh(range(len(author_counts)), author_counts.values)\n",
    "        axes[1, 0].set_yticks(range(len(author_counts)))\n",
    "        axes[1, 0].set_yticklabels(author_counts.index)\n",
    "        axes[1, 0].set_title('ì£¼ìš” ì €ì (ìƒìœ„ 10ëª…)')\n",
    "        axes[1, 0].set_xlabel('ë…¼ë¬¸ ìˆ˜')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'ì €ì ë°ì´í„° ì—†ìŒ', ha='center', va='center')\n",
    "        axes[1, 0].set_title('ì£¼ìš” ì €ì')\n",
    "    \n",
    "    # 4. í‚¤ì›Œë“œ ë¶„ì„ (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ì„)\n",
    "    if 'keywords' in df.columns and df['keywords'].notna().sum() > 0:\n",
    "        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ê³„ì‚°\n",
    "        all_keywords = []\n",
    "        for keywords in df['keywords'].dropna():\n",
    "            if isinstance(keywords, str):\n",
    "                words = [word.strip() for word in keywords.split(',')]\n",
    "                all_keywords.extend(words)\n",
    "        \n",
    "        if all_keywords:\n",
    "            from collections import Counter\n",
    "            keyword_counts = Counter(all_keywords)\n",
    "            top_keywords = dict(keyword_counts.most_common(10))\n",
    "            \n",
    "            axes[1, 1].barh(range(len(top_keywords)), list(top_keywords.values()))\n",
    "            axes[1, 1].set_yticks(range(len(top_keywords)))\n",
    "            axes[1, 1].set_yticklabels(list(top_keywords.keys()))\n",
    "            axes[1, 1].set_title('ì£¼ìš” í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ)')\n",
    "            axes[1, 1].set_xlabel('ë¹ˆë„')\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'í‚¤ì›Œë“œ ë¶„ì„ ë¶ˆê°€', ha='center', va='center')\n",
    "            axes[1, 1].set_title('ì£¼ìš” í‚¤ì›Œë“œ')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'í‚¤ì›Œë“œ ë°ì´í„° ì—†ìŒ', ha='center', va='center')\n",
    "        axes[1, 1].set_title('ì£¼ìš” í‚¤ì›Œë“œ')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ‰ í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"âœ… ì´ ìˆ˜ì§‘ ë¬¸ì„œ ìˆ˜: {len(df)}ê°œ\")\n",
    "    print(f\"ğŸ“‚ ì €ì¥ íŒŒì¼: {filename if 'filename' in locals() else 'N/A'}\")\n",
    "    print(f\"ğŸ• ì‘ì—… ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if 'type' in df.columns:\n",
    "        print(f\"\\nğŸ“Š ë¬¸ì„œ íƒ€ì…ë³„ ìš”ì•½:\")\n",
    "        for doc_type, count in df['type'].value_counts().items():\n",
    "            print(f\"  - {doc_type}: {count}ê°œ\")\n",
    "else:\n",
    "    print(\"âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ API ì ‘ê·¼ ë¬¸ì œë‚˜ ë„¤íŠ¸ì›Œí¬ ì´ìŠˆì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ“ ì‘ì—… ë¡œê·¸:\")\n",
    "print(f\"  - API í…ŒìŠ¤íŠ¸: {'ì„±ê³µ' if 'api_available' in locals() and api_available else 'ì‹¤íŒ¨'}\")\n",
    "print(f\"  - ë°ì´í„° ìˆ˜ì§‘: {'ì„±ê³µ' if 'unique_data' in locals() and unique_data else 'ì‹¤íŒ¨'}\")\n",
    "print(f\"  - ì—‘ì…€ ì €ì¥: {'ì„±ê³µ' if 'filename' in locals() else 'ì‹¤íŒ¨'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08f1a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì£¼ìš” í‚¤ì›Œë“œ (ë¹ˆë„ìˆ˜ í¬í•¨):\n",
      "laser: 153\n",
      "high: 123\n",
      "hydrogen: 85\n",
      "plasma: 81\n",
      "air: 77\n",
      "via: 75\n",
      "heat: 72\n",
      "thermal: 70\n",
      "gas: 70\n",
      "characteristics: 69\n",
      "machine: 63\n",
      "micro: 55\n",
      "temperature: 52\n",
      "pump: 52\n",
      "energy: 52\n",
      "roll: 51\n",
      "technology: 50\n",
      "ion: 49\n",
      "control: 49\n",
      "ë”°ë¥¸: 48\n",
      "ê°œë°œdevelopment: 47\n",
      "fuel: 46\n",
      "sensor: 45\n",
      "measurement: 45\n",
      "transfer: 45\n",
      "liquid: 45\n",
      "manufacturing: 44\n",
      "ì—°êµ¬a: 43\n",
      "3d: 43\n",
      "film: 43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "\n",
    "# path to csv\n",
    "csv_path = r'D:\\simu\\TB\\Modelica_Modelon_KETI\\TwinBuilder_Modelon\\KIMM_rt_20250829_174129_y2023_2025.csv'\n",
    "df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "\n",
    "# 2. ëª¨ë“  ë…¼ë¬¸ ì œëª©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤.\n",
    "text = ' '.join(df['title'].astype(str))\n",
    "\n",
    "# 3. í•œê¸€ ë° ì˜ë¬¸ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì •ê·œí‘œí˜„ì‹ (ìˆ«ì í¬í•¨í•œ ì—°ì† ë¬¸ìì—´)\n",
    "pattern = re.compile(r\"[A-Za-z0-9\\uac00-\\ud7a3]+\")\n",
    "tokens = pattern.findall(text)\n",
    "\n",
    "# 4. ë¶ˆìš©ì–´ ëª©ë¡: ì¡°ì‚¬, ì¼ë°˜ ë™ì‚¬, í‰ê°€/ë¶„ì„/ëª¨ë¸ ë“± ì—°êµ¬ ì£¼ì œì™€ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ë‹¨ì–´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "stopwords = {\n",
    "    'and','the','for','with','using','use','in','of','to','a','on','as','by','an',\n",
    "    'at','from','this','that','is','are','be','into','based','study','analysis',\n",
    "    'model','design','development','evaluation','case','report','experiment','properties',\n",
    "    'ì—°êµ¬','ê¸°ìˆ ','ê°œë°œ','ëª¨ë¸','ë¶„ì„','í‰ê°€','ì„±ëŠ¥','íŠ¹ì„±',\n",
    "    'ì´ìš©í•œ','í™œìš©í•œ','ì´ìš©','í™œìš©','ì‚¬ìš©í•œ','ì‚¬ìš©','ìœ„í•œ','ê´€í•œ','ê¸°ë°˜','ê´€ë ¨',\n",
    "    'í…ŒìŠ¤íŠ¸','ì‹œí—˜','ë„ì…','ì œì¡°','ì œì‘','ë°©ë²•','ë°©ë²•ë¡ ','ì‹¤í—˜','ì˜ˆì¸¡','ì¸¡ì •',\n",
    "    'process','performance','system','method','methods','effect','effects','result','results',\n",
    "    # í•œê¸€ ì¡°ì‚¬\n",
    "    'ì˜','ì„','ë¥¼','ê³¼','ë°','ì—','ëŠ”','ê°€','ë„','ë¡œ','ì—ì„œ','ë“±','ì¤‘','í•œ','ê²ƒ','ì´ë‹¤'\n",
    "}\n",
    "\n",
    "# 5. í† í°ì„ í•„í„°ë§í•©ë‹ˆë‹¤: ìˆ«ìë§Œ ë˜ëŠ” ê¸¸ì´ê°€ 1ì¸ í† í°, ë¶ˆìš©ì–´ë¥¼ ì œì™¸í•©ë‹ˆë‹¤.\n",
    "def is_valid(tok):\n",
    "    return len(tok) > 1 and not tok.isdigit()\n",
    "\n",
    "filtered_tokens = [\n",
    "    tok.lower() for tok in tokens\n",
    "    if is_valid(tok) and tok.lower() not in stopwords\n",
    "]\n",
    "\n",
    "# 6. ë¹ˆë„ìˆ˜ ê³„ì‚° í›„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "counter = Counter(filtered_tokens)\n",
    "top_keywords = counter.most_common(30)  # ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ ì¶œë ¥\n",
    "\n",
    "# 7. ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ì£¼ìš” í‚¤ì›Œë“œ (ë¹ˆë„ìˆ˜ í¬í•¨):\")\n",
    "for word, freq in top_keywords:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
